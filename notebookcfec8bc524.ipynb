{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2b93f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.276099,
     "end_time": "2023-03-28T03:38:41.245616",
     "exception": false,
     "start_time": "2023-03-28T03:38:38.969517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108867c",
   "metadata": {
    "papermill": {
     "duration": 0.005025,
     "end_time": "2023-03-28T03:38:41.258199",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.253174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1497e",
   "metadata": {
    "papermill": {
     "duration": 0.004946,
     "end_time": "2023-03-28T03:38:41.268377",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.263431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## HED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998149f3",
   "metadata": {
    "papermill": {
     "duration": 0.149444,
     "end_time": "2023-03-28T03:38:41.423279",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.273835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HED(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.line1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.line2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.line3 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.line4 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.line5 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.line1_out = torch.nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "        self.line2_out = torch.nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "        self.line3_out = torch.nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "        self.line4_out = torch.nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "        self.line5_out = torch.nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=5, out_channels=1, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X * 255.0\n",
    "\n",
    "        X1 = self.line1(X)\n",
    "        X2 = self.line2(X1)\n",
    "        X3 = self.line3(X2)\n",
    "        X4 = self.line4(X3)\n",
    "        X5 = self.line5(X4)\n",
    "\n",
    "        output_1 = self.line1_out(X1)\n",
    "        output_2 = self.line2_out(X2)\n",
    "        output_3 = self.line3_out(X3)\n",
    "        output_4 = self.line4_out(X4)\n",
    "        output_5 = self.line5_out(X5)\n",
    "\n",
    "        output_1 = torch.nn.functional.interpolate(input=output_1, size=(X.shape[2], X.shape[3]), mode='bilinear', align_corners=False)\n",
    "        output_2 = torch.nn.functional.interpolate(input=output_2, size=(X.shape[2], X.shape[3]), mode='bilinear', align_corners=False)\n",
    "        output_3 = torch.nn.functional.interpolate(input=output_3, size=(X.shape[2], X.shape[3]), mode='bilinear', align_corners=False)\n",
    "        output_4 = torch.nn.functional.interpolate(input=output_4, size=(X.shape[2], X.shape[3]), mode='bilinear', align_corners=False)\n",
    "        output_5 = torch.nn.functional.interpolate(input=output_5, size=(X.shape[2], X.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return self.output(torch.cat([output_1, output_2, output_3, output_4, output_5], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4c1fd",
   "metadata": {
    "papermill": {
     "duration": 0.005417,
     "end_time": "2023-03-28T03:38:41.434007",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.428590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Edge UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130a7b0",
   "metadata": {
    "papermill": {
     "duration": 0.016437,
     "end_time": "2023-03-28T03:38:41.455553",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.439116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.block(X)\n",
    "    \n",
    "class DoubleCNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "        super(DoubleCNNBlock, self).__init__()\n",
    "        self.block1 = CNNBlock(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.block2 = CNNBlock(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.block1(X)\n",
    "        X = self.block2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9ccaa",
   "metadata": {
    "papermill": {
     "duration": 0.021279,
     "end_time": "2023-03-28T03:38:41.482072",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.460793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Expansion(nn.Module):\n",
    "    def __init__(self, in_channels, ratio, kernel_size=1, stride=1, padding=0):\n",
    "        super(Expansion, self).__init__()\n",
    "        self.out_channels = in_channels * ratio\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.block(X)\n",
    "    \n",
    "class DepthwiseCNN(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=3, stride=1, padding=0):\n",
    "        super(DepthwiseCNN, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.block(X)\n",
    "    \n",
    "class Compression(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "        super(Compression, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.block(X)\n",
    "    \n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ratio, kernel_size, stride=1, padding=1):\n",
    "        super(MBConv, self).__init__()\n",
    "        self.expanssion_block = Expansion(in_channels, ratio)\n",
    "        self.depthwise_cnn_block = DepthwiseCNN(in_channels * ratio, kernel_size=3, padding=1)\n",
    "        self.compression_block = Compression(in_channels * ratio, out_channels)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.expanssion_block(X)\n",
    "        X = self.depthwise_cnn_block(X)\n",
    "        X = self.compression_block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9371116",
   "metadata": {
    "papermill": {
     "duration": 0.021406,
     "end_time": "2023-03-28T03:38:41.508491",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.487085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.double_cnn_block = DoubleCNNBlock(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.max_pool_layer = nn.MaxPool2d(2, 2)\n",
    "        self.dropout_layer = nn.Dropout2d(p=0.3, inplace=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.double_cnn_block(X)\n",
    "        X = self.max_pool_layer(out)\n",
    "        X = self.dropout_layer(X)\n",
    "        return X, out\n",
    "    \n",
    "class DownSampleMBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ratio, kernel_size, stride=1, padding=1):\n",
    "        super(DownSampleMBConv, self).__init__()\n",
    "        self.mbconv_block = MBConv(in_channels, out_channels, ratio, kernel_size)\n",
    "        self.max_pool_layer = nn.MaxPool2d(2, 2)\n",
    "        self.dropout_layer = nn.Dropout2d(p=0.3, inplace=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.mbconv_block(X)\n",
    "        X = self.max_pool_layer(out)\n",
    "        X = self.dropout_layer(X)\n",
    "        return X, out\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels, in_channels//2, kernel_size=1, padding=0)\n",
    "        self.dropout_layer = nn.Dropout2d(p=0.3)\n",
    "        self.conv_in = (in_channels * 3) // 2\n",
    "        self.double_conv_block = DoubleCNNBlock(self.conv_in, out_channels, padding=1)\n",
    "        \n",
    "    def forward(self, X, out, E):\n",
    "        X = F.interpolate(X, out.shape[2:], mode='bilinear', align_corners=False)\n",
    "        X = self.conv_layer(X)\n",
    "        mul = torch.mul(out, E)\n",
    "        X = torch.concat([mul, out, X], dim=1)\n",
    "        X = self.dropout_layer(X)\n",
    "        X = self.double_conv_block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1360a",
   "metadata": {
    "papermill": {
     "duration": 0.019555,
     "end_time": "2023-03-28T03:38:41.533557",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.514002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.down_sample_block1 = DownSample(1, filters[0])\n",
    "        self.down_sample_block2 = DownSampleMBConv(filters[0], filters[1], 2, kernel_size=3)\n",
    "        self.down_sample_block3 = DownSampleMBConv(filters[1], filters[2], 4, kernel_size=3)\n",
    "        self.down_sample_block4 = DownSampleMBConv(filters[2], filters[3], 4, kernel_size=3)\n",
    "        self.bottleneck_block = DoubleCNNBlock(filters[3], filters[4], padding=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X, out1 = self.down_sample_block1(X)\n",
    "        X, out2 = self.down_sample_block2(X)\n",
    "        X, out3 = self.down_sample_block3(X)\n",
    "        X, out4 = self.down_sample_block4(X)\n",
    "        bottleneck = self.bottleneck_block(X)\n",
    "        return out1, out2, out3, out4, bottleneck\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.up_sample_block4 = UpSample(filters[4], filters[3])\n",
    "        self.up_sample_block3 = UpSample(filters[3], filters[2])\n",
    "        self.up_sample_block2 = UpSample(filters[2], filters[1])\n",
    "        self.up_sample_block1 = UpSample(filters[1], filters[0])\n",
    "        \n",
    "    def forward(self, out1, out2, out3, out4, E1, E2, E3, E4, bottleneck):\n",
    "        X = self.up_sample_block4(bottleneck, out4, E4)\n",
    "        X = self.up_sample_block3(X, out3, E3)\n",
    "        X = self.up_sample_block2(X, out2, E2)\n",
    "        X = self.up_sample_block1(X, out1, E1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88a3bc",
   "metadata": {
    "papermill": {
     "duration": 0.017328,
     "end_time": "2023-03-28T03:38:41.555998",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.538670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EDGEUnet(nn.Module):\n",
    "    def __init__(self, num_classes, encoder_filters, decoder_filters):\n",
    "        super(EDGEUnet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.encoder_filters = encoder_filters\n",
    "        self.decoder_filters = decoder_filters\n",
    "        self.hed_model = HED()\n",
    "        self.encoder = Encoder(encoder_filters)\n",
    "        self.decoder = Decoder(decoder_filters)\n",
    "        self.output_conv_layer = nn.Conv2d(decoder_filters[0], num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        edge = self.hed_model(X)\n",
    "        out1, out2, out3, out4, bottleneck = self.encoder(X)\n",
    "        \n",
    "        E1 = F.interpolate(edge, out1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        E2 = F.interpolate(edge, out2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        E3 = F.interpolate(edge, out3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        E4 = F.interpolate(edge, out4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        X = self.decoder(out1, out2, out3, out4, E1, E2, E3, E4, bottleneck)\n",
    "        return self.output_conv_layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458f0b5",
   "metadata": {
    "papermill": {
     "duration": 5.264776,
     "end_time": "2023-03-28T03:38:46.826317",
     "exception": false,
     "start_time": "2023-03-28T03:38:41.561541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "filters = (64, 128, 256, 512, 1024)\n",
    "NUM_CLASSES = 3\n",
    "model = EDGEUnet(NUM_CLASSES, filters, filters).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/edge-unet/last_epoch-00.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4e549",
   "metadata": {
    "papermill": {
     "duration": 0.004994,
     "end_time": "2023-03-28T03:38:46.836941",
     "exception": false,
     "start_time": "2023-03-28T03:38:46.831947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54062e",
   "metadata": {
    "papermill": {
     "duration": 0.940864,
     "end_time": "2023-03-28T03:38:47.783117",
     "exception": false,
     "start_time": "2023-03-28T03:38:46.842253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/gitractcsv/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9954fe",
   "metadata": {
    "papermill": {
     "duration": 0.020447,
     "end_time": "2023-03-28T03:38:47.812340",
     "exception": false,
     "start_time": "2023-03-28T03:38:47.791893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_img(path):\n",
    "    img = Image.open(path)\n",
    "    img = np.expand_dims(np.array(img), axis=-1).astype('float32')\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img /= mx\n",
    "    return img\n",
    "\n",
    "def load_mask(path):\n",
    "    mask = np.load(path).astype('float32')\n",
    "    mask /= 255.\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1412e8c",
   "metadata": {
    "papermill": {
     "duration": 0.024752,
     "end_time": "2023-03-28T03:38:47.845329",
     "exception": false,
     "start_time": "2023-03-28T03:38:47.820577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UWMGIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        row = df.loc[i]\n",
    "        img = load_img(row.image_path)\n",
    "        \n",
    "        if self.label:\n",
    "            mask = load_mask(row.mask_path)\n",
    "            if self.transforms:\n",
    "                data = self.transforms(image=img, mask=mask)\n",
    "                img, mask = data['image'], data['mask']\n",
    "            img = np.transpose(img, (-1, 0, 1))\n",
    "            mask = np.transpose(mask, (-1, 0, 1))\n",
    "            return torch.tensor(img), torch.tensor(mask)\n",
    "        else:\n",
    "            if self.transforms:\n",
    "                data = self.transforms(image=img)\n",
    "                img = data['image']\n",
    "            img = np.transpose(img, (-1, 0, 1))\n",
    "            return torch.tensor(img)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0465a6",
   "metadata": {
    "papermill": {
     "duration": 1.907851,
     "end_time": "2023-03-28T03:38:49.761332",
     "exception": false,
     "start_time": "2023-03-28T03:38:47.853481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "IMG_SIZE = [320, 384]\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": None,\n",
    "    \"valid\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69cd884",
   "metadata": {
    "papermill": {
     "duration": 0.035623,
     "end_time": "2023-03-28T03:38:49.807265",
     "exception": false,
     "start_time": "2023-03-28T03:38:49.771642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "train_dataset = UWMGIDataset(train_df, label=True, transforms=data_transforms[\"train\"])\n",
    "valid_dataset = UWMGIDataset(valid_df, label=True, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ba735",
   "metadata": {
    "papermill": {
     "duration": 16.468924,
     "end_time": "2023-03-28T03:39:06.282118",
     "exception": false,
     "start_time": "2023-03-28T03:38:49.813194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -qq install segmentation_models_pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050e1b9",
   "metadata": {
    "papermill": {
     "duration": 0.020492,
     "end_time": "2023-03-28T03:39:06.312387",
     "exception": false,
     "start_time": "2023-03-28T03:39:06.291895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DiceLoss    = smp.losses.DiceLoss(mode='multilabel')\n",
    "BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    return 0.5*BCELoss(y_pred, y_true) + 0.5*DiceLoss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8af9cd",
   "metadata": {
    "papermill": {
     "duration": 0.019058,
     "end_time": "2023-03-28T03:39:06.336768",
     "exception": false,
     "start_time": "2023-03-28T03:39:06.317710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_ACCUMULATIONS = 2\n",
    "from tqdm import tqdm\n",
    "from torch.cuda import amp\n",
    "\n",
    "def train(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
    "    for step, (images, masks) in pbar:         \n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        masks  = masks.to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with amp.autocast(enabled=True):\n",
    "            y_pred = model(images)\n",
    "            loss   = criterion(y_pred, masks)\n",
    "            loss   = loss / N_ACCUMULATIONS\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "        if (step + 1) % N_ACCUMULATIONS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da47276",
   "metadata": {
    "papermill": {
     "duration": 0.018769,
     "end_time": "2023-03-28T03:39:06.361118",
     "exception": false,
     "start_time": "2023-03-28T03:39:06.342349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    val_scores = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid')\n",
    "    for step, (images, masks) in pbar:        \n",
    "        images  = images.to(device, dtype=torch.float)\n",
    "        masks   = masks.to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        y_pred  = model(images)\n",
    "        loss    = criterion(y_pred, masks)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        y_pred = nn.Sigmoid()(y_pred)\n",
    "        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n",
    "        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n",
    "        val_scores.append([val_dice, val_jaccard])\n",
    "        \n",
    "    val_scores  = np.mean(val_scores, axis=0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5f12f",
   "metadata": {
    "papermill": {
     "duration": 0.018915,
     "end_time": "2023-03-28T03:39:06.385350",
     "exception": false,
     "start_time": "2023-03-28T03:39:06.366435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    \n",
    "    \n",
    "    best_model_wts = None\n",
    "    best_dice      = -np.inf\n",
    "    best_epoch     = -1\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_dataloader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        \n",
    "        val_loss, val_scores = valid_one_epoch(model, valid_dataloader, \n",
    "                                                 device=device, \n",
    "                                                 epoch=epoch)\n",
    "        val_dice, val_jaccard = val_scores\n",
    "        \n",
    "        if val_dice >= best_dice:\n",
    "            best_dice    = val_dice\n",
    "            best_jaccard = val_jaccard\n",
    "            best_epoch   = epoch\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"best_epoch-{fold:02d}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            \n",
    "        last_model_wts = copy.deepcopy(model.state_dict())\n",
    "        PATH = f\"last_epoch-{fold:02d}.bin\"\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8cdc6",
   "metadata": {
    "papermill": {
     "duration": 0.015855,
     "end_time": "2023-03-28T03:39:06.406859",
     "exception": false,
     "start_time": "2023-03-28T03:39:06.391004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be9b48",
   "metadata": {
    "papermill": {
     "duration": 31925.145184,
     "end_time": "2023-03-28T12:31:11.557713",
     "exception": false,
     "start_time": "2023-03-28T03:39:06.412529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_training(model, optimizer, scheduler=None, device=device, num_epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31966.781799,
   "end_time": "2023-03-28T12:31:16.288726",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-28T03:38:29.506927",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
